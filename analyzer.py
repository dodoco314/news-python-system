import json
from janome.tokenizer import Tokenizer
from collections import Counter
import re

# ITæŠ€è¡“ç”¨èªè¾æ›¸
TECH_KEYWORDS = {
    # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
    'Python', 'JavaScript', 'TypeScript', 'Java', 'Go', 'Rust', 'C++', 'C#', 'PHP', 
    'Ruby', 'Swift', 'Kotlin', 'Dart', 'Scala', 'R', 'SQL',
    
    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
    'React', 'Vue', 'Angular', 'Next.js', 'Nuxt', 'Svelte', 'jQuery', 'HTML', 'CSS',
    'Tailwind', 'Bootstrap', 'Webpack', 'Vite',
    
    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
    'Node.js', 'Express', 'Django', 'Flask', 'FastAPI', 'Spring', 'Rails',
    'Laravel', 'ASP.NET',
    
    # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰
    'Docker', 'Kubernetes', 'k8s', 'AWS', 'GCP', 'Azure', 'Firebase', 
    'Vercel', 'Netlify', 'Heroku', 'Terraform', 'Ansible',
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Elasticsearch', 'DynamoDB',
    'Oracle', 'SQLite', 'MariaDB',
    
    # AIãƒ»æ©Ÿæ¢°å­¦ç¿’
    'AI', 'ChatGPT', 'Claude', 'Gemini', 'GPT', 'LLM', 'TensorFlow', 'PyTorch',
    'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'æ·±å±¤å­¦ç¿’', 'è‡ªç„¶è¨€èªå‡¦ç†', 'NLP',
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
    'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'è„†å¼±æ€§', 'XSS', 'CSRF', 'SQL', 'OAuth', 'JWT', 'HTTPS',
    
    # é–‹ç™ºãƒ„ãƒ¼ãƒ«
    'GitHub', 'GitLab', 'Git', 'VSCode', 'IntelliJ', 'Vim', 'Docker', 'CI/CD',
    'GitHub Actions', 'Jenkins',
    
    # WebæŠ€è¡“
    'API', 'REST', 'GraphQL', 'WebSocket', 'HTTP', 'gRPC', 'JSON', 'XML',
    
    # ãƒ¢ãƒã‚¤ãƒ«
    'iOS', 'Android', 'Flutter', 'React Native', 'SwiftUI',
    
    # ãã®ä»–æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
    'ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹', 'ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹', 'ã‚³ãƒ³ãƒ†ãƒŠ', 'ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«', 'DevOps',
    'CI', 'CD', 'ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™º', 'TDD', 'ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–', 'Edge',
    
    # ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å
    'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Twitter', 'X',
    'OpenAI', 'Anthropic', 'Notion', 'Slack', 'Discord', 'Figma',
    'Cloudflare', 'Stripe'
}

def load_json(filename="hatena_ranking.json"):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    """
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    except json.JSONDecodeError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {filename} ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
        return []

def extract_tech_keywords(articles):
    """
    è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡º
    """
    tokenizer = Tokenizer()
    keywords = []
    
    print("\nğŸ” æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­...\n")
    
    for article in articles:
        title = article.get("title", "")
        
        # ã¾ãšã‚¿ã‚¤ãƒˆãƒ«å…¨ä½“ã‹ã‚‰æŠ€è¡“ç”¨èªè¾æ›¸ã«å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’æ¢ã™
        for tech_word in TECH_KEYWORDS:
            # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«æ¤œç´¢
            if tech_word.lower() in title.lower() or tech_word in title:
                keywords.append(tech_word)
        
        # æ¬¡ã«å½¢æ…‹ç´ è§£æã§åè©ã‚’æŠ½å‡ºã—ã€æŠ€è¡“ç”¨èªè¾æ›¸ã¨ç…§åˆ
        for token in tokenizer.tokenize(title):
            parts = str(token).split("\t")
            if len(parts) >= 2:
                word = parts[0]
                pos = parts[1].split(",")[0]
                
                # åè©ã§ã€æŠ€è¡“ç”¨èªè¾æ›¸ã«å«ã¾ã‚Œã‚‹ã‚‚ã®
                if pos == "åè©" and word in TECH_KEYWORDS:
                    keywords.append(word)
    
    return keywords

def analyze_trending_words(keywords, top_n=15):
    """
    é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŒ–
    """
    counter = Counter(keywords)
    ranking = counter.most_common(top_n)
    
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ ITæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰ TOP{top_n} ğŸ”¥ğŸ”¥ğŸ”¥\n")
    print("-" * 60)
    
    if not ranking:
        print("âš ï¸  æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return []
    
    for rank, (word, count) in enumerate(ranking, 1):
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«çš„ã«è¦‹ã‚„ã™ã
        bar = "â–ˆ" * min(count, 20)  # æœ€å¤§20æ–‡å­—
        print(f"{rank:2d}ä½: {word:20s} {bar} ({count}å›)")
    
    print("-" * 60)
    
    return ranking

def save_trending_words(ranking, filename="trending_words.json"):
    """
    æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ã‚’JSONã§ä¿å­˜
    """
    data = [{"rank": i+1, "word": word, "count": count} 
            for i, (word, count) in enumerate(ranking)]
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… {filename} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

def analyze_from_json(json_filename="hatena_ranking.json", top_n=15):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§åˆ†æï¼ˆå¤–éƒ¨ã‹ã‚‰å‘¼ã³å‡ºã—ç”¨ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š ITæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰åˆ†æé–‹å§‹")
    print("=" * 60)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    articles = load_json(json_filename)
    
    if not articles:
        print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"\nğŸ“š {len(articles)}ä»¶ã®è¨˜äº‹ã‚’åˆ†æã—ã¾ã™")
    
    # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = extract_tech_keywords(articles)
    
    print(f"âœ… {len(keywords)}å€‹ã®æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã—ãŸ\n")
    
    # æ€¥ä¸Šæ˜‡ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    ranking = analyze_trending_words(keywords, top_n=top_n)
    
    # JSONã«ä¿å­˜
    if ranking:
        save_trending_words(ranking)
    
    print("\nğŸ‰ åˆ†æå®Œäº†ï¼\n")
    
    return ranking

def main():
    """
    å˜ä½“å®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    analyze_from_json()

if __name__ == "__main__":
    main()